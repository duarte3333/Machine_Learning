{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train = np.load('Xtrain_Classification1.npy') #6254 x 2352. 2352 = pixels x pixels x 3 (RGB). Training set is inbalanced, different number of samples for each class\n",
    "y_train = np.load('ytrain_Classification1.npy') #6254. 1D vector\n",
    "X_test = np.load('Xtest_Classification1.npy') #1764 x 2352. Has data from two distinct sourcesprint(X_train[0:10,0:2], y_train[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should scaler down the values of the training data by dividing each value with 255 (highest value of \"brightness\" of pixel) since classifiers can be sensitive to feature scaling. In this case, we will standardize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) #apply same transformation to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define possible values of alpha, the regularization term in the MLP classifier function that prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambdas(begin, end, increment):\n",
    "    \"\"\" Creates a list of hyperparameter values for cross-validation. \"\"\"\n",
    "    return [begin + i * increment for i in range(int((end - begin) / increment))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_list = create_lambdas(0.1, 10, 1)\n",
    "lrate_init = create_lambdas(0.0001, 0.001, 0.0001)\n",
    "parameters = {'alpha': alphas_list} #To add to GridSearchCV we need to create a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaodias/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "classifier = MLPClassifier(solver= 'adam') #Keep default solver 'adam' since is better for larger datasets and is robust\n",
    "grid_search = GridSearchCV(classifier, parameters) \n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs to be reshaped. If N is number of samples np.reshape(X,(N,28,28,3))\n",
    "1- Build\n",
    "Prof says we should define layers sequentialy.\n",
    "We need to define activation function (ReLU, tanh, sigmoid...)\n",
    "In the last layer we define depending on what we want. For regression we use linear, for classification we use nonlinear (sigmoid, softmax). Softmax needs to have as many units as classes in dataset.\n",
    "\n",
    "2-Train\n",
    "    -Define cost function(MSE for regression, Categorical cross entropy - binary cross entropy)\n",
    "    -Define optimizer (sgd, momentum, adaptive steps...) One very common is adam\n",
    "    -Define initial learning rate\n",
    "    -Nº epochs. If we use train in mini-batch we need to define batch size.\n",
    "    -We split data into train and validation data. We should plot the evolution of the loss along the epochs for both train data and validation data. The lines have the same shape but the loss is normally higher for the validation data. NOTE: If loss starts to increase suddently at a certain epoch in validation data then we have an overfitted network. It's for this reason that we should use an early stopping callback (stops if loss starts growing after a certain epochs). \n",
    "    -For the batch size just choose one we are happy with and computer doesn't take a lot to run. Don't try to find the optimal value.\n",
    "\n",
    "3 - Predictions\n",
    "    w_hat = arg max(y_hat)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
