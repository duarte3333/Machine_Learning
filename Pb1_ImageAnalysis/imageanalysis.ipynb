{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train = np.load('Xtrain_Classification1.npy') #6254 x 2352. 2352 = pixels x pixels x 3 (RGB). Training set is inbalanced, different number of samples for each class\n",
    "y_train = np.load('ytrain_Classification1.npy') #6254. 1D vector\n",
    "X_test = np.load('Xtest_Classification1.npy') #1764 x 2352. Has data from two distinct sourcesprint(X_train[0:10,0:2], y_train[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should scaler down the values of the training data by dividing each value with 255 (highest value of \"brightness\" of pixel) since classifiers can be sensitive to feature scaling. In this case, we will standardize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) #apply same transformation to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define possible values of alpha, the regularization term in the MLP classifier function that prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambdas(begin, end, increment):\n",
    "    \"\"\" Creates a list of hyperparameter values for cross-validation. \"\"\"\n",
    "    return [begin + i * increment for i in range(int((end - begin) / increment))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_list = create_lambdas(0.1, 10, 1)\n",
    "lrate_init = create_lambdas(0.0001, 0.001, 0.0001)\n",
    "parameters = {'alpha': [0.5, 1, 1.5, 2], 'learning_rate_init': [0.0001, 0.0005, 0.001, 0.01, 0.1]} #To add to GridSearchCV we need to create a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MLPClassifier(solver= 'adam') #Keep default solver 'adam' since is better for larger datasets and is robust\n",
    "grid_search = GridSearchCV(classifier, parameters) \n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
